@article{richardson1972,
	author = {Richardson, William Hadley},
	journal = {J. Opt. Soc. Am.},
	keywords = {Crosstalk; Deconvolution; Image processing; Image restoration; Imaging techniques; Point spread function},
	number = {1},
	pages = {55--59},
	publisher = {Optica Publishing Group},
	title = {Bayesian-based iterative method of image restoration},
	volume = {62},
	month = {Jan},
	year = {1972},
	url = {https://opg.optica.org/abstract.cfm?URI=josa-62-1-55},
	doi = {10.1364/JOSA.62.000055},
	abstract = {An iterative method of restoring degraded images was developed by treating images, point spread functions, and degraded images as probability-frequency functions and by applying Bayes's theorem. The method functions effectively in the presence of noise and is adaptable to computer operation.},
}

@Book{venables2002,
	Author = {Venables, W. N. and Ripley, B. D.},
	Title = {Modern applied statistics with {S}},
	Edition = {4th ed.},
	FSeries = {Statistics and Computing (Cham)},
	Series = {Stat. Comput. (Cham)},
	ISSN = {1431-8784},
	ISBN = {0-387-95457-0; 978-1-4419-3190-0; 978-0-387-21824-3},
	Year = {2002},
	Publisher = {New York, NY: Springer},
	DOI = {10.1007/b97626},
	Keywords = {62-01,62-04,65C60,62-00,62-07,68U99},
	zbMATH = {1813698},
	Zbl = {1006.62003}
}

@article{patterson2006,
	doi = {10.1371/journal.pgen.0020190},
	author = {Patterson, Nick AND Price, Alkes L. AND Reich, David},
	journal = {PLOS Genetics},
	publisher = {Public Library of Science},
	title = {Population Structure and Eigenanalysis},
	year = {2006},
	month = {12},
	volume = {2},
	url = {https://doi.org/10.1371/journal.pgen.0020190},
	pages = {1--20},
	abstract = {Current methods for inferring population structure from genetic data do not provide formal significance tests for population differentiation. We discuss an approach to studying population structure (principal components analysis) that was first applied to genetic data by Cavalli-Sforza and colleagues. We place the method on a solid statistical footing, using results from modern statistics to develop formal significance tests. We also uncover a general “phase change” phenomenon about the ability to detect structure in genetic data, which emerges from the statistical theory we use, and has an important implication for the ability to discover structure in genetic data: for a fixed but large dataset size, divergence between two populations (as measured, for example, by a statistic like FST) below a threshold is essentially undetectable, but a little above threshold, detection will be easy. This means that we can predict the dataset size needed to detect structure.},
	number = {12}
}

@article{kumar2015,
	author = {Kumar, Satish and Molloy, Claire and Muñoz, Patricio and Daetwyler, Hans and Chagné, David and Volz, Richard},
	title = {Genome-enabled estimates of additive and nonadditive genetic variances and prediction of apple phenotypes across environments},
	journal = {G3 Genes|Genomes|Genetics},
	volume = {5},
	number = {12},
	pages = {2711--2718},
	year = {2015},
	month = {12},
	abstract = "{The nonadditive genetic effects may have an important contribution to total genetic variation of phenotypes, so estimates of both the additive and nonadditive effects are desirable for breeding and selection purposes. Our main objectives were to: estimate additive, dominance and epistatic variances of apple (Malus × domestica Borkh.) phenotypes using relationship matrices constructed from genome-wide dense single nucleotide polymorphism (SNP) markers; and compare the accuracy of genomic predictions using genomic best linear unbiased prediction models with or without including nonadditive genetic effects. A set of 247 clonally replicated individuals was assessed for six fruit quality traits at two sites, and also genotyped using an Illumina 8K SNP array. Across several fruit quality traits, the additive, dominance, and epistatic effects contributed about 30\\%, 16\\%, and 19\\%, respectively, to the total phenotypic variance. Models ignoring nonadditive components yielded upwardly biased estimates of additive variance (heritability) for all traits in this study. The accuracy of genomic predicted genetic values (GEGV) varied from about 0.15 to 0.35 for various traits, and these were almost identical for models with or without including nonadditive effects. However, models including nonadditive genetic effects further reduced the bias of GEGV. Between-site genotypic correlations were high (\\&gt;0.85) for all traits, and genotype-site interaction accounted for \\&lt;10\\% of the phenotypic variability. The accuracy of prediction, when the validation set was present only at one site, was generally similar for both sites, and varied from about 0.50 to 0.85. The prediction accuracies were strongly influenced by trait heritability, and genetic relatedness between the training and validation families.}",
	issn = {2160-1836},
	doi = {10.1534/g3.115.021105},
	url = {https://doi.org/10.1534/g3.115.021105},
	eprint = {https://academic.oup.com/g3journal/article-pdf/5/12/2711/40571424/g3journal2711.pdf},
}



@article{strumbelj2024,
	author = {\v{S}trumbelj, Erik and Bouchard-C{\^o}t{\'e}, Alexandre and Corander, Jukka and  Gelman, Andrew and Rue, H{\aa}vard  and  Murray, Lawrence and  Pesonen, Henri and  Plummer, Martyn and  Vehtari, Aki},
	title = {{Past, present and future of software for Bayesian inference}},
	volume = {39},
	journal = {Statistical Science},
	number = {1},
	publisher = {Institute of Mathematical Statistics},
	pages = {46 -- 61},
	keywords = {computation, data analysis, MCMC, probabilistic programming, statistics},
	year = {2024},
	doi = {10.1214/23-STS907},
	URL = {https://doi.org/10.1214/23-STS907}
}


@inproceedings{ding2005,
	author = {Ding, Chris and He, Xiaofeng and Simon, Horst D.},
	year = {2005},
	title = {{On the equivalence of Nonnegative Matrix Factorization and spectral clustering}},
	booktitle = {{Proceedings of the 2005 SIAM International Conference on Data Mining (SDM)}},
	chapter = {},
	pages = {606--610},
	doi = {10.1137/1.9781611972757.70},
	publisher = {Society for Industrial and Applied Mathematics},
	address = {Philadelphia, PA},
	editor = {Kargupta, Hillol and Kamath, Chandrika and Srivastava, Jaideep and Goodman, Arnold},
	URL = {https://epubs.siam.org/doi/abs/10.1137/1.9781611972757.70},
	eprint = {https://epubs.siam.org/doi/pdf/10.1137/1.9781611972757.70},
	abstract = {Abstract Current nonnegative matrix factorization (NMF) deals with X = FGT type. We provide a systematic analysis and extensions of NMF to the symmetric W = HHT, and the weighted W = HSHT. We show that (1) W = HHT is equivalent to Kernel if-means clustering and the Laplacian-based spectral clustering. (2) X = FGT is equivalent to simultaneous clustering of rows and columns of a bipartite graph. Algorithms are given for computing these symmetric NMFs. }
}

@inproceedings{mnih2007,
	author = {Mnih, Andriy and Salakhutdinov, Russ R.},
	booktitle = {{Advances in Neural Information Processing Systems}},
	editor = {J. Platt and D. Koller and Y. Singer and S. Roweis},
	pages = {},
	publisher = {Curran Associates, Inc.},
	title = {Probabilistic matrix factorization},
	url = {https://proceedings.neurips.cc/paper_files/paper/2007/file/d7322ed717dedf1eb4e6e52a37ea7bcd-Paper.pdf},
	volume = {20},
	year = {2007}
}

@Book{ripley1996,
	Author = {Ripley, B. D.},
	Title = {Pattern recognition and neural networks},
	ISBN = {0-521-46086-7},
	Year = {1996},
	Publisher = {Cambridge: Cambridge Univ. Press},
	Language = {English},
	Keywords = {62H30,68T10,62-01,68T05,68-01,62C05},
	zbMATH = {835699},
	Zbl = {0853.62046}
}

@article{olshausen1997,
	title = {{Sparse coding with an overcomplete basis set: A strategy employed by V1?}},
	journal = {Vision Research},
	volume = {37},
	number = {23},
	pages = {3311--3325},
	year = {1997},
	issn = {0042-6989},
	doi = {https://doi.org/10.1016/S0042-6989(97)00169-7},
	url = {https://www.sciencedirect.com/science/article/pii/S0042698997001697},
	author = {Olshausen, Bruno A. and Field, David J.},
	keywords = {Coding, V1, Gabor-wavelet, Natural images},
	abstract = {The spatial receptive fields of simple cells in mammalian striate cortex have been reasonably well described physiologically and can be characterized as being localized, oriented, and bandpass, comparable with the basis functions of wavelet transforms. Previously, we have shown that these receptive field properties may be accounted for in terms of a strategy for producing a sparse distribution of output activity in response to natural images. Here, in addition to describing this work in a more expansive fashion, we examine the neurobiological implications of sparse coding. Of particular interest is the case when the code is overcomplete—i.e., when the number of code elements is greater than the effective dimensionality of the input space. Because the basis functions are non-orthogonal and not linearly independent of each other, sparsifying the code will recruit only those basis functions necessary for representing a given input, and so the input-output function will deviate from being purely linear. These deviations from linearity provide a potential explanation for the weak forms of non-linearity observed in the response properties of cortical simple cells, and they further make predictions about the expected interactions among units in response to naturalistic stimuli.}
}

@article{godsill2001,
	author = {Godsill, Simon J.},
	title = {{On the relationship between Markov Chain Monte Carlo methods for model uncertainty}},
	journal = {Journal of Computational and Graphical Statistics},
	volume = {10},
	number = {2},
	pages = {230--248},
	year = {2001},
	publisher = {Taylor \& Francis},
	doi = {10.1198/10618600152627924},
	
	
	URL = { 
	
	https://doi.org/10.1198/10618600152627924
	
	
	
	},
	eprint = { 
	
	https://doi.org/10.1198/10618600152627924
	
	
	
	}
	
}

@article{green95,
	author = {Green, Peter J.},
	title = {{Reversible jump Markov chain Monte Carlo computation and Bayesian model determination}},
	journal = {Biometrika},
	volume = {82},
	number = {4},
	pages = {711--732},
	year = {1995},
	month = {12},
	abstract = "{Markov chain Monte Carlo methods for Bayesian computation have until recently been restricted to problems where the joint distribution of all variables has a density with respect to some fixed standard underlying measure. They have therefore not been available for application to Bayesian model determination, where the dimensionality of the parameter vector is typically not fixed. This paper proposes a new framework for the construction of reversible Markov chain samplers that jump between parameter subspaces of differing dimensionality, which is flexible and entirely constructive. It should therefore have wide applicability in model determination problems. The methodology is illustrated with applications to multiple change-point analysis in one and two dimensions, and to a Bayesian comparison of binomial experiments.}",
	issn = {0006-3444},
	doi = {10.1093/biomet/82.4.711},
	url = {https://doi.org/10.1093/biomet/82.4.711},
	eprint = {https://academic.oup.com/biomet/article-pdf/82/4/711/699533/82-4-711.pdf},
}

@article{hoyer2004,
	author = {Hoyer, Patrik O.},
	title = {{Non-negative Matrix Factorization with sparseness constraints}},
	year = {2004},
	issue_date = {12/1/2004},
	publisher = {JMLR.org},
	volume = {5},
	issn = {1532-4435},
	abstract = {Non-negative matrix factorization (NMF) is a recently developed technique for finding parts-based, linear representations of non-negative data. Although it has successfully been applied in several applications, it does not always result in parts-based representations. In this paper, we show how explicitly incorporating the notion of 'sparseness' improves the found decompositions. Additionally, we provide complete MATLAB code both for standard NMF and for our extension. Our hope is that this will further the application of these methods to solving novel data-analysis problems.},
	journal = {J. Mach. Learn. Res.},
	month = {dec},
	pages = {1457--1469},
	numpages = {13}
}



@article{field1994,
	author = {Field, David J.},
	title = {{What is the goal of sensory coding?}},
	journal = {Neural Computation},
	volume = {6},
	number = {4},
	pages = {559--601},
	year = {1994},
	month = {07},
	abstract = "{A number of recent attempts have been made to describe early sensory coding in terms of a general information processing strategy. In this paper, two strategies are contrasted. Both strategies take advantage of the redundancy in the environment to produce more effective representations. The first is described as a “compact” coding scheme. A compact code performs a transform that allows the input to be represented with a reduced number of vectors (cells) with minimal RMS error. This approach has recently become popular in the neural network literature and is related to a process called Principal Components Analysis (PCA). A number of recent papers have suggested that the optimal “compact” code for representing natural scenes will have units with receptive field profiles much like those found in the retina and primary visual cortex. However, in this paper, it is proposed that compact coding schemes are insufficient to account for the receptive field properties of cells in the mammalian visual pathway. In contrast, it is proposed that the visual system is near to optimal in representing natural scenes only if optimality is defined in terms of “sparse distributed” coding. In a sparse distributed code, all cells in the code have an equal response probability across the class of images but have a low response probability for any single image. In such a code, the dimensionality is not reduced. Rather, the redundancy of the input is transformed into the redundancy of the firing pattern of cells. It is proposed that the signature for a sparse code is found in the fourth moment of the response distribution (i.e., the kurtosis). In measurements with 55 calibrated natural scenes, the kurtosis was found to peak when the bandwidths of the visual code matched those of cells in the mammalian visual cortex. Codes resembling “wavelet transforms” are proposed to be effective because the response histograms of such codes are sparse (i.e., show high kurtosis) when presented with natural scenes. It is proposed that the structure of the image that allows sparse coding is found in the phase spectrum of the image. It is suggested that natural scenes, to a first approximation, can be considered as a sum of self-similar local functions (the inverse of a wavelet). Possible reasons for why sensory systems would evolve toward sparse coding are presented.}",
	issn = {0899-7667},
	doi = {10.1162/neco.1994.6.4.559},
	url = {https://doi.org/10.1162/neco.1994.6.4.559},
	eprint = {https://direct.mit.edu/neco/article-pdf/6/4/559/812786/neco.1994.6.4.559.pdf},
}

@article{paatero1994,
	author = {Paatero, Pentti and Tapper, Unto},
	title = {{Positive matrix factorization: A non-negative factor model with optimal utilization of error estimates of data values}},
	journal = {Environmetrics},
	volume = {5},
	number = {2},
	pages = {111--126},
	keywords = {Factor analysis, Principal component analysis, Weighted least squares, Alternating regression, Error estimates, Scaling, Repetitive measurements},
	doi = {https://doi.org/10.1002/env.3170050203},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/env.3170050203},
	eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1002/env.3170050203},
	abstract = {Abstract A new variant ‘PMF’ of factor analysis is described. It is assumed that X is a matrix of observed data and σ is the known matrix of standard deviations of elements of X. Both X and σ are of dimensions n × m. The method solves the bilinear matrix problem X = GF + E where G is the unknown left hand factor matrix (scores) of dimensions n × p, F is the unknown right hand factor matrix (loadings) of dimensions p × m, and E is the matrix of residuals. The problem is solved in the weighted least squares sense: G and F are determined so that the Frobenius norm of E divided (element-by-element) by σ is minimized. Furthermore, the solution is constrained so that all the elements of G and F are required to be non-negative. It is shown that the solutions by PMF are usually different from any solutions produced by the customary factor analysis (FA, i.e. principal component analysis (PCA) followed by rotations). Usually PMF produces a better fit to the data than FA. Also, the result of PF is guaranteed to be non-negative, while the result of FA often cannot be rotated so that all negative entries would be eliminated. Different possible application areas of the new method are briefly discussed. In environmental data, the error estimates of data can be widely varying and non-negativity is often an essential feature of the underlying models. Thus it is concluded that PMF is better suited than FA or PCA in many environmental applications. Examples of successful applications of PMF are shown in companion papers.},
	year = {1994}
}



@article{lee1999,
	abstract = {Is perception of the whole based on perception of its parts? There is psychological1 and physiological2,3 evidence for parts-based representations in the brain, and certain computational theories of object recognition rely on such representations4,5. But little is known about how brains or computers might learn the parts of objects. Here we demonstrate an algorithm for non-negative matrix factorization that is able to learn parts of faces and semantic features of text. This is in contrast to other methods, such as principal components analysis and vector quantization, that learn holistic, not parts-based, representations. Non-negative matrix factorization is distinguished from the other methods by its use of non-negativity constraints. These constraints lead to a parts-based representation because they allow only additive, not subtractive, combinations. When non-negative matrix factorization is implemented as a neural network, parts-based representations emerge by virtue of two properties: the firing rates of neurons are never negative and synaptic strengths do not change sign.},
	author = {Lee, Daniel D. and Seung, H. Sebastian},
	date = {1999/10/01},
	date-added = {2024-05-17 23:55:35 +0200},
	date-modified = {2024-05-17 23:55:35 +0200},
	doi = {10.1038/44565},
	id = {Lee1999},
	isbn = {1476-4687},
	journal = {Nature},
	number = {6755},
	pages = {788--791},
	title = {Learning the parts of objects by non-negative matrix factorization},
	url = {https://doi.org/10.1038/44565},
	volume = {401},
	year = {1999},
	bdsk-url-1 = {https://doi.org/10.1038/44565}}
	

@article{cemgil2009,
	abstract = {We describe nonnegative matrix factorisation (NMF) with a Kullback-Leibler (KL) error measure in a statisticalframework, with a hierarchical generative model consisting of an observation and a prior component. Omitting the priorleads to the standard KL-NMF algorithms as special cases, where maximum likelihood parameter estimation is carriedout via the Expectation-Maximisation (EM) algorithm. Starting from this view, we develop full Bayesian inferencevia variational Bayes or Monte Carlo. Our construction retains conjugacy and enables us to develop more powerfulmodels while retaining attractive features of standard NMF such as monotonic convergence and easy implementation.We illustrate our approach on model order selection and image reconstruction.},
	author = {Cemgil, Ali Taylan},
	date = {2009/05/27},
	date-added = {2024-05-18 10:38:29 +0200},
	date-modified = {2024-05-18 10:38:29 +0200},
	doi = {10.1155/2009/785152},
	isbn = {1687-5265},
	journal = {Computational Intelligence and Neuroscience},
	pages = {785152},
	publisher = {Hindawi Publishing Corporation},
	title = {Bayesian Inference for Nonnegative Matrix Factorisation Models},
	url = {https://doi.org/10.1155/2009/785152},
	volume = {2009},
	year = {2009},
	bdsk-url-1 = {https://doi.org/10.1155/2009/785152}}


@INPROCEEDINGS{virtanen2008,
	author={Virtanen, Tuomas and Taylan Cemgil, A. and Godsill, Simon},
	booktitle={2008 IEEE International Conference on Acoustics, Speech and Signal Processing}, 
	title={Bayesian extensions to non-negative matrix factorisation for audio signal modelling}, 
	year={2008},
	volume={},
	number={},
	pages={1825--1828},
	keywords={Bayesian methods;Instruments;Signal processing;Time frequency analysis;Signal generators;Source separation;Spectrogram;Multiple signal classification;Music;Laboratories;acoustic signal processing;matrix decomposition;MAP estimation;source separation},
	doi={10.1109/ICASSP.2008.4517987}}
	
@Book{murphy2022,
	Author = {Murphy, Kevin P.},
	Title = {Probabilistic machine learning. {An} introduction},
	FSeries = {Adaptive Computation and Machine Learning},
	Series = {Adapt. Comput. Mach. Learn.},
	ISBN = {978-0-262-04682-4},
	Year = {2022},
	Publisher = {Cambridge, MA: MIT Press},
	Keywords = {68-01,62C12,68T05,68T07},
	URL = {mitpress.mit.edu/books/probabilistic-machine-learning},
	zbMATH = {7475764},
	Zbl = {1498.68002}
}

@Book{strang2000,
	Author = {Strang, Gilbert},
	Title = {Introduction to linear algebra.},
	Edition = {5th edition},
	ISBN = {978-0-9802327-7-6},
	Year = {2016},
	Publisher = {Wellesley, MA: Wellesley-Cambridge Press},
	Keywords = {15-01,15A03,15A04,15A06,65Fxx,15A15,15A18,15A63,15B51,90C05,42A20,65D18},
	URL = {math.mit.edu/~gs/linearalgebra/},
	zbMATH = {6607459},
	Zbl = {1351.15002}
}

@article{blei2017,
	author = {Blei, David M. and Kucukelbir, Alp and McAuliffe, Jon D.},
	title = {Variational Inference: A Review for Statisticians},
	journal = {Journal of the American Statistical Association},
	volume = {112},
	number = {518},
	pages = {859--877},
	year = {2017},
	publisher = {Taylor \& Francis},
	doi = {10.1080/01621459.2017.1285773},
	URL = { 
	https://doi.org/10.1080/01621459.2017.1285773
	},
	eprint = { 
	https://doi.org/10.1080/01621459.2017.1285773
	}
}

@article{lindsten2013,
	url = {http://dx.doi.org/10.1561/2200000045},
	year = {2013},
	volume = {6},
	journal = {Foundations and Trends{\texttrademark} in Machine Learning},
	title = {{Backward Simulation Methods for Monte Carlo Statistical Inference}},
	doi = {10.1561/2200000045},
	issn = {1935-8237},
	number = {1},
	pages = {1-143},
	author = {Lindsten, Fredrik and Sch\"{o}n, Thomas B.}
}

@article{carpenter2017,
	title={{Stan: a probabilistic programming language}},
	volume={76},
	url={https://www.jstatsoft.org/index.php/jss/article/view/v076i01},
	doi={10.18637/jss.v076.i01},
	abstract={Stan is a probabilistic programming language for specifying statistical models. A Stan program imperatively defines a log probability function over parameters conditioned on specified data and constants. As of version 2.14.0, Stan provides full Bayesian inference for continuous-variable models through Markov chain Monte Carlo methods such as the No-U-Turn sampler, an adaptive form of Hamiltonian Monte Carlo sampling. Penalized maximum likelihood estimates are calculated using optimization methods such as the limited memory Broyden-Fletcher-Goldfarb-Shanno algorithm. Stan is also a platform for computing log densities and their gradients and Hessians, which can be used in alternative algorithms such as variational Bayes, expectation propagation, and marginal inference using approximate integration. To this end, Stan is set up so that the densities, gradients, and Hessians, along with intermediate quantities of the algorithm such as acceptance probabilities, are easily accessible. Stan can be called from the command line using the cmdstan package, through R using the rstan package, and through Python using the pystan package. All three interfaces support sampling and optimization-based inference with diagnostics and posterior analysis. rstan and pystan also provide access to log probabilities, gradients, Hessians, parameter transforms, and specialized plotting.},
	number={1},
	journal={Journal of Statistical Software},
	author={Carpenter, Bob and Gelman, Andrew and Hoffman, Matthew D. and Lee, Daniel and Goodrich, Ben and Betancourt, Michael and Brubaker, Marcus and Guo, Jiqiang and Li, Peter and Riddell, Allen},
	year={2017},
	pages={1--32}
}

@Manual{stan2024,
	title = {Stan modeling language user's guide and reference manual. Version 2.34. https://mc-stan.org},
	author = {{Stan Development Team}},
	year = {2024},
	url = {https://mc-stan.org},
}

@Book{bishop2006,
	Author = {Bishop, Christopher M.},
	Title = {Pattern recognition and machine learning.},
	FSeries = {Information Science and Statistics},
	Series = {Inf. Sci. Stat.},
	ISSN = {1613-9011},
	ISBN = {0-387-31073-8},
	Year = {2006},
	Publisher = {New York, NY: Springer},
	Keywords = {68T01,68T05,68T10,68-01},
	zbMATH = {5046468},
	Zbl = {1107.68072}
}


@InProceedings{carvalho2009,
	title = 	 {Handling Sparsity via the Horseshoe},
	author = 	 {Carvalho, Carlos M. and Polson, Nicholas G. and Scott, James G.},
	booktitle = 	 {Proceedings of the Twelth International Conference on Artificial Intelligence and Statistics},
	pages = 	 {73--80},
	year = 	 {2009},
	editor = 	 {van Dyk, David and Welling, Max},
	volume = 	 {5},
	series = 	 {Proceedings of Machine Learning Research},
	address = 	 {Hilton Clearwater Beach Resort, Clearwater Beach, Florida USA},
	month = 	 {16--18 Apr},
	publisher =    {PMLR},
	pdf = 	 {http://proceedings.mlr.press/v5/carvalho09a/carvalho09a.pdf},
	url = 	 {https://proceedings.mlr.press/v5/carvalho09a.html},
	abstract = 	 {This paper presents a general, fully Bayesian framework for sparse supervised-learning problems based on the horseshoe prior. The horseshoe prior is a member of the family of multivariate scale mixtures of normals, and is therefore closely related to widely used approaches for sparse Bayesian learning, including, among others, Laplacian priors (e.g. the LASSO) and Student-t priors (e.g. the relevance vector machine). The advantages of the horseshoe are its robustness at handling unknown sparsity and large outlying signals. These properties are justifed theoretically via a representation theorem and accompanied by comprehensive empirical experiments that compare its performance to benchmark alternatives.}
}

@inproceedings{kucukelbir2015,
	author = {Kucukelbir, Alp and Ranganath, Rajesh and Gelman, Andrew and Blei, David},
	booktitle = {Advances in Neural Information Processing Systems},
	editor = {Cortes, C.  and Lawrence, N.  and Lee, D.  and Sugiyama, M.  and Garnett, R. },
	publisher = {Curran Associates, Inc.},
	title = {Automatic variational inference in {Stan}},
	url = {https://proceedings.neurips.cc/paper_files/paper/2015/file/352fe25daf686bdb4edca223c921acea-Paper.pdf},
	volume = {28},
	year = {2015}
}

@article{kucukelbir2017,
	author  = {Kucukelbir, Alp and Tran, Dustin and Ranganath, Rajesh and Gelman, Andrew  and Blei, David M.},
	title   = {{Automatic differentiation variational inference}},
	journal = {Journal of Machine Learning Research},
	year    = {2017},
	volume  = {18},
	number  = {14},
	pages   = {1--45},
	url     = {http://jmlr.org/papers/v18/16-107.html}
}

@Book{murphy2023,
	Author = {Murphy, Kevin P.},
	Title = {Probabilistic machine learning. {Advanced} topics},
	FSeries = {Adaptive Computation and Machine Learning},
	Series = {Adapt. Comput. Mach. Learn.},
	ISBN = {978-0-262-04843-9; 978-0-262-37600-6},
	Year = {2023},
	Publisher = {Cambridge, MA: MIT Press},
	Keywords = {68-01,68T05,68T07},
	URL = {probml.github.io/pml-book/book2.html},
	zbMATH = {7646020},
	Zbl = {1521.68004}
}


@article{mackay1995,
	doi = {10.1088/0954-898X/6/3/011},
	url = {https://dx.doi.org/10.1088/0954-898X/6/3/011},
	year = {1995},
	month = {aug},
	publisher = {},
	volume = {6},
	number = {3},
	pages = {469},
	author = {MacKay, D. J. C.},
	title = {Probable networks and plausible predictions-a review of practical {B}ayesian methods for supervised neural networks},
	journal = {Network: Computation in Neural Systems},
	abstract = {Bayesian probability theory provides a unifying framework for data modelling. In this framework the overall aims are to find models that are well matched to the data, and to use these models to make optimal predictions. Neural network learning is interpreted as an inference of the most probable parameters for the model, given the training data. The search in model space (i.e., the space of architectures, noise models, preprocessings, regularizers and weight decay constants) can then also be treated as an inference problem, in which we infer the relative probability of alternative models, given the data. The article describes practical techniques based on Gaussian approximations for implementation of these powerful methods for controlling, comparing and using adaptive networks.}
}

@Book{mackay2003,
	Author = {MacKay, David J. C.},
	Title = {Information theory, inference and learning algorithms.},
	ISBN = {0-521-64298-1},
	Year = {2003},
	Publisher = {Cambridge: Cambridge University Press},
	Keywords = {94-01,62F10,62H30,68Mxx,68P30},
	zbMATH = {2061729},
	Zbl = {1055.94001}
}